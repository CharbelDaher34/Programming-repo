{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-05T05:51:07.201460Z","iopub.execute_input":"2022-08-05T05:51:07.202426Z","iopub.status.idle":"2022-08-05T05:51:07.214982Z","shell.execute_reply.started":"2022-08-05T05:51:07.202372Z","shell.execute_reply":"2022-08-05T05:51:07.214113Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain.set_index('PassengerId',inplace=True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:07.313530Z","iopub.execute_input":"2022-08-05T05:51:07.314728Z","iopub.status.idle":"2022-08-05T05:51:07.339280Z","shell.execute_reply.started":"2022-08-05T05:51:07.314677Z","shell.execute_reply":"2022-08-05T05:51:07.338466Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest.set_index('PassengerId',inplace=True)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:07.433539Z","iopub.execute_input":"2022-08-05T05:51:07.434237Z","iopub.status.idle":"2022-08-05T05:51:07.456243Z","shell.execute_reply.started":"2022-08-05T05:51:07.434197Z","shell.execute_reply":"2022-08-05T05:51:07.455402Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:07.561262Z","iopub.execute_input":"2022-08-05T05:51:07.562112Z","iopub.status.idle":"2022-08-05T05:51:07.570518Z","shell.execute_reply.started":"2022-08-05T05:51:07.562052Z","shell.execute_reply":"2022-08-05T05:51:07.569416Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"markdown","source":"**Now i'll give real integers to each different value in the 'Embarked' and the 'Sex' columns**\n\n**Here i gave the 2 entries with 'nan' values a value 0 in the 'Embarked' columns**","metadata":{}},{"cell_type":"code","source":"train['Embarked']=train['Embarked'].apply(lambda i:str(i))\nprint('Number of nan entries:',len(train[train['Embarked']=='nan']))\ntrain['Embarked'].replace({'S':1,'Q':2,'C':3,'nan':0},inplace=True)\ntrain['Sex']=train['Sex'].replace({'female':0,'male':1})\n","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:07.677748Z","iopub.execute_input":"2022-08-05T05:51:07.678153Z","iopub.status.idle":"2022-08-05T05:51:07.692759Z","shell.execute_reply.started":"2022-08-05T05:51:07.678121Z","shell.execute_reply":"2022-08-05T05:51:07.691885Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"markdown","source":"**We have here the 'Cabin' column where the entries are the name of the cabin, but there is ones with 'nan' values so i replaced it with 'NoCabin' and ill do onehotencoding using the sklearn onehotencoder and apply it also on the test set.**","metadata":{}},{"cell_type":"code","source":"\n\ntrain['Cabin']=train['Cabin'].apply(lambda x:str(x))\ntrain['Cabin'].replace({'nan':'NoCabin'},inplace=True)\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe1=OneHotEncoder(categories='auto',  drop=None, sparse=True, handle_unknown='ignore' )      \ntransformed = ohe1.fit_transform(train[['Cabin']])\nx=pd.DataFrame(transformed.toarray())\nx.columns=ohe1.categories_\nx.index=train.index\ntrain=train.join(x).drop(['Cabin'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:07.787725Z","iopub.execute_input":"2022-08-05T05:51:07.788554Z","iopub.status.idle":"2022-08-05T05:51:07.809461Z","shell.execute_reply.started":"2022-08-05T05:51:07.788509Z","shell.execute_reply":"2022-08-05T05:51:07.808311Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"markdown","source":"**Now i'll train a second one hot encoder on the 'Ticket' column**","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe2=OneHotEncoder(categories='auto',  drop=None, sparse=True, handle_unknown='ignore' )      \ntransformed = ohe2.fit_transform(train[['Ticket']])\nx=pd.DataFrame(transformed.toarray())\nx.columns=ohe2.categories_\nx.index=train.index\ntrain=train.join(x).drop(['Ticket'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:07.898778Z","iopub.execute_input":"2022-08-05T05:51:07.899234Z","iopub.status.idle":"2022-08-05T05:51:07.926343Z","shell.execute_reply.started":"2022-08-05T05:51:07.899197Z","shell.execute_reply":"2022-08-05T05:51:07.925021Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"markdown","source":"**The name won't affect the result i guess and it's unique for every person so it should not be corrolated to the 'Survived' column, so i dropped it**","metadata":{}},{"cell_type":"code","source":"\ntrain.drop(['Name'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:08.003564Z","iopub.execute_input":"2022-08-05T05:51:08.004814Z","iopub.status.idle":"2022-08-05T05:51:08.015193Z","shell.execute_reply.started":"2022-08-05T05:51:08.004772Z","shell.execute_reply":"2022-08-05T05:51:08.014106Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"markdown","source":"**In the 'Age'column ,there is some missing values so i replaced it with the mean value of the column**","metadata":{}},{"cell_type":"code","source":"\nimport math\nmean_value=math.floor(np.mean(train['Age']))+1\ntrain['Age'].fillna(value=mean_value, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:08.116154Z","iopub.execute_input":"2022-08-05T05:51:08.117205Z","iopub.status.idle":"2022-08-05T05:51:08.123985Z","shell.execute_reply.started":"2022-08-05T05:51:08.117160Z","shell.execute_reply":"2022-08-05T05:51:08.122793Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:08.240299Z","iopub.execute_input":"2022-08-05T05:51:08.240703Z","iopub.status.idle":"2022-08-05T05:51:08.285825Z","shell.execute_reply.started":"2022-08-05T05:51:08.240672Z","shell.execute_reply":"2022-08-05T05:51:08.284996Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"y_train=train['Survived']\nX_train=train.drop(['Survived'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:08.354382Z","iopub.execute_input":"2022-08-05T05:51:08.355016Z","iopub.status.idle":"2022-08-05T05:51:08.362581Z","shell.execute_reply.started":"2022-08-05T05:51:08.354981Z","shell.execute_reply":"2022-08-05T05:51:08.361535Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"markdown","source":"**Now i'll do the same preprocessing to the 'test' set as if they are coming from the same dataset as the 'train' dataset so the same mistakes.**\n\n**And ill apply the one hot encoders trained on the 'train' dataset on the test dataset**","metadata":{}},{"cell_type":"code","source":"test['Fare'].fillna(value=np.mean(test['Fare']),inplace=True)\ntest['Embarked']=test['Embarked'].apply(lambda i:str(i))\nprint('Number of nan entries:',len(test[test['Embarked']=='nan']))\ntest['Embarked'].replace({'S':1,'Q':2,'C':3,'nan':0},inplace=True)\ntest['Sex']=test['Sex'].replace({'female':0,'male':1})\ntest['Cabin']=test['Cabin'].apply(lambda x:str(x))\ntest['Cabin'].replace({'nan':'NoCabin'},inplace=True)\ntransformed = ohe1.transform(test[['Cabin']])\nx=pd.DataFrame(transformed.toarray())\nx.columns=ohe1.categories_\nx.index=test.index\ntest=test.join(x).drop(['Cabin'],axis=1)\ntransformed = ohe2.transform(test[['Ticket']])\nx=pd.DataFrame(transformed.toarray())\nx.columns=ohe2.categories_\nx.index=test.index\ntest=test.join(x).drop(['Ticket'],axis=1)\ntest.drop(['Name'],axis=1,inplace=True)\nimport math\nmean_value=math.floor(np.mean(test['Age']))+1\ntest['Age'].fillna(value=mean_value, inplace=True)\ntest\n","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:51:08.542063Z","iopub.execute_input":"2022-08-05T05:51:08.542721Z","iopub.status.idle":"2022-08-05T05:51:08.622117Z","shell.execute_reply.started":"2022-08-05T05:51:08.542684Z","shell.execute_reply":"2022-08-05T05:51:08.620900Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf2 = GradientBoostingClassifier()\ngb_clf2.fit(X_train, y_train)\npredictions = gb_clf2.predict(test)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-05T05:54:08.385529Z","iopub.execute_input":"2022-08-05T05:54:08.386007Z","iopub.status.idle":"2022-08-05T05:54:09.220672Z","shell.execute_reply.started":"2022-08-05T05:54:08.385969Z","shell.execute_reply":"2022-08-05T05:54:09.219397Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"output=pd.DataFrame(predictions)\noutput.columns=['Survived']\noutput.set_index(test.index,inplace=True)\noutput","metadata":{"execution":{"iopub.status.busy":"2022-08-05T06:03:33.077174Z","iopub.execute_input":"2022-08-05T06:03:33.077566Z","iopub.status.idle":"2022-08-05T06:03:33.090430Z","shell.execute_reply.started":"2022-08-05T06:03:33.077534Z","shell.execute_reply":"2022-08-05T06:03:33.089458Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"output.to_csv('CharbelTitanicProject')","metadata":{"execution":{"iopub.status.busy":"2022-08-05T06:03:45.727768Z","iopub.execute_input":"2022-08-05T06:03:45.728204Z","iopub.status.idle":"2022-08-05T06:03:45.736305Z","shell.execute_reply.started":"2022-08-05T06:03:45.728168Z","shell.execute_reply":"2022-08-05T06:03:45.734969Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-08-05T06:03:25.262428Z","iopub.execute_input":"2022-08-05T06:03:25.262821Z","iopub.status.idle":"2022-08-05T06:03:25.271004Z","shell.execute_reply.started":"2022-08-05T06:03:25.262790Z","shell.execute_reply":"2022-08-05T06:03:25.270155Z"},"trusted":true},"execution_count":212,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}